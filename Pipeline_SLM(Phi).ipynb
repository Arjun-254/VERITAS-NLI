{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from scraping_selenium import people_also_ask\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_dataset(FINAL).csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article Pipeline Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_links(query, num_links=1):\n",
    "    linksgot = []\n",
    "    try:\n",
    "        # Perform Google search and get the top links\n",
    "        search_results = search(query, num_results=num_links)\n",
    "\n",
    "        # Print the top links\n",
    "        for i, link in enumerate(search_results, start=1):\n",
    "            linksgot.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return linksgot\n",
    "\n",
    "\n",
    "def get_title_and_content(search_query_results):\n",
    "    article_titles = []\n",
    "    article_content = []\n",
    "\n",
    "    # Set up Selenium options (headless mode if needed)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument('log-level=1')\n",
    "\n",
    "    cService = Service(executable_path='PATH_TO_SELENIUM_WEBDRIVER')\n",
    "    driver = webdriver.Chrome(service=cService,options=chrome_options)\n",
    "\n",
    "    if search_query_results:\n",
    "        for result in search_query_results:\n",
    "            try:\n",
    "                # Navigate to the URL\n",
    "                driver.get(result)\n",
    "\n",
    "                # Scrape <h1> tags for titles\n",
    "                h1_tags = driver.find_elements(By.TAG_NAME, 'h1')\n",
    "                currenth1 = \" \".join([h1.text for h1 in h1_tags])\n",
    "                article_titles.append(currenth1)\n",
    "\n",
    "                # Scrape <p> tags for content\n",
    "                p_tags = driver.find_elements(By.TAG_NAME, 'p')\n",
    "                currentp = \" \".join([p.text for p in p_tags])\n",
    "                article_content.append(currentp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Quit the driver'\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "\n",
    "    return article_titles, article_content\n",
    "\n",
    "\n",
    "# Get the titles and contents\n",
    "def make_data(search_query_results):\n",
    "    titles, contents = get_title_and_content(\n",
    "        get_top_links(search_query_results))\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data = {'Title': titles, 'Content': contents}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load FactCC Model pipeline\n",
    "pipe = pipeline(model=\"manueldeprada/FactCC\", task=\"text-classification\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To answer the question generated by SLM via google\n",
    "def google_search(query):\n",
    "    headers = {\n",
    "        'User-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Perform the Google search\n",
    "    query = re.sub(r'[^a-zA-Z0-9.,!?;:()\\- ]+', '', query)\n",
    "    search_url = f'https://www.google.com/search?q={query}'\n",
    "    html = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    # Parse the HTML response\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    \n",
    "    #extract answer from summary answer on google\n",
    "    if soup.select_one('.hgKElc'):\n",
    "        answer = soup.select_one('.hgKElc').text \n",
    "\n",
    "    # Extract the answer (assuming it's in a specific class) -> QUICK ANSWER BOX\n",
    "    elif soup.select_one('.DI6Ufb'):\n",
    "        answer = soup.select_one('.DI6Ufb')\n",
    "        answer = answer.find(class_='Z0LcW t2b5Cf').text\n",
    "        answer=query+' '+answer\n",
    "    \n",
    "    elif not soup.select_one('.DI6Ufb'):\n",
    "        result=people_also_ask(search_url)\n",
    "        # print(\"Result of the Scraping\",result)\n",
    "        if len(result)==2:\n",
    "            answer=f'{result[0]} {result[1]}'\n",
    "        else:\n",
    "            #incase people also asked also comes empty we revert to the article pipeline\n",
    "            scraped_df = make_data(query)\n",
    "            scraped_df.dropna(inplace=True)\n",
    "\n",
    "            # Convert the 'Content' column to strings\n",
    "            scraped_df['Content'] = scraped_df['Content'].astype(str)\n",
    "\n",
    "            # Sort the DataFrame based on the length of the strings in the 'Content' column\n",
    "            scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False)\n",
    "\n",
    "            if len(scraped_df) == 0 or not scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Title'][0] :\n",
    "                #Could not retrieve articles related to headline, Could possibly be a false claim OR Scraper got blocked/forbidden\n",
    "                return str('Cannot retrieve articles, most likely to be false')\n",
    "\n",
    "            scraped_content = (\n",
    "            f\"{scraped_df['Title'][0]} \\n{scraped_df['Content'][0]}\")\n",
    "            answer = scraped_content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FactCC_Phi(input_headline ,Phi_generated_question):\n",
    "    quickSearchAnswer = google_search(Phi_generated_question)\n",
    "\n",
    "    #compare scraped info as source and headline as the claim\n",
    "    ans = pipe([[[quickSearchAnswer,Phi_generated_question]]], truncation=True, padding='max_length')\n",
    "\n",
    "    print('Input headline : ', input_headline)\n",
    "    print('Phi-3 Generated Question : ', Phi_generated_question)\n",
    "    print('Scraped Answer : ', quickSearchAnswer)\n",
    "    if ans[0]['label'] == 'CORRECT':\n",
    "        return True,quickSearchAnswer\n",
    "    else:\n",
    "        return False,quickSearchAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decision,scraped_content = FactCC_Phi(df['Headline'][0],df['Question_phi'][0])\n",
    "actual_decision = df['Label'][0]\n",
    "print(model_decision,actual_decision)\n",
    "if (model_decision) == (actual_decision):\n",
    "    print('Correct Decision')\n",
    "else:\n",
    "    print('Incorrect Decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "predictions = []\n",
    "actuals = []\n",
    "results = []\n",
    "batch_size = 2\n",
    "csv_filename = 'Pipeline_SLM(Phi).csv'\n",
    "processed_headlines = set()\n",
    "\n",
    "# Check if CSV file already exists and load processed headlines\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter='|')\n",
    "        next(csvreader)  # Skip header\n",
    "        for row in csvreader:\n",
    "            if len(row) < 2:  # Skip rows that may not have all columns\n",
    "                continue\n",
    "            headline = row[1]\n",
    "            headline = headline.str.strip(\"'\\\"\")\n",
    "            headline = headline.lower()\n",
    "            processed_headlines.add(headline)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open the CSV file for writing with '|' as the delimiter\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='|')\n",
    "        if not processed_headlines:\n",
    "            csvwriter.writerow(['Index', 'Headline','Scraped_Content','Question_phi', 'Model Decision', 'Actual Decision'])\n",
    "        \n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in tqdm(df.iterrows(),total= len(df)):\n",
    "            try:\n",
    "                headline = row['Headline']\n",
    "                if headline in processed_headlines:\n",
    "                    print(f\"Skipping already processed headline: {headline}\")\n",
    "                    continue\n",
    "                model_decision,scraped_content = FactCC_Phi(row['Headline'], row['Question_phi'])\n",
    "                # Convert scraped content to a single line\n",
    "                scraped_content_single_line = ' '.join(scraped_content.splitlines()).strip().replace('|', ' ')\n",
    "                actual_decision = row['Label']\n",
    "                predictions.append(model_decision)\n",
    "                actuals.append(actual_decision)\n",
    "                \n",
    "                # Write the results to the CSV file\n",
    "                results.append([index, row['Headline'], scraped_content_single_line, row['Question_phi'],model_decision, actual_decision])\n",
    "                \n",
    "                # print(f\"Index: {index}, Model Decision: {model_decision}, Actual Decision: {actual_decision}\")\n",
    "                # print(f\"------------------------------------------------------------------------------------\")\n",
    "                time.sleep(5) \n",
    "                if len(results) % batch_size == 0:\n",
    "                        csvwriter.writerows(results)\n",
    "                        csvfile.flush()\n",
    "                        os.fsync(csvfile.fileno())\n",
    "                        results = [] # Delay to prevent rate limiting\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Write any remaining results\n",
    "        if results:\n",
    "            csvwriter.writerows(results)\n",
    "            csvfile.flush()\n",
    "            os.fsync(csvfile.fileno())\n",
    "\n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    precision = precision_score(actuals, predictions, pos_label=True)\n",
    "    recall = recall_score(actuals, predictions, pos_label=True)\n",
    "    f1 = f1_score(actuals, predictions, pos_label=True)\n",
    "\n",
    "    # Append the metrics to the CSV file\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='|')\n",
    "        csvwriter.writerow([])  # Add an empty row for separation\n",
    "        csvwriter.writerow(['Metric', 'Value'])\n",
    "        csvwriter.writerow(['Accuracy', accuracy])\n",
    "        csvwriter.writerow(['Precision', precision])\n",
    "        csvwriter.writerow(['Recall', recall])\n",
    "        csvwriter.writerow(['F1 Score', f1])\n",
    "\n",
    "    print(f'Results written to {csv_filename}')\n",
    "    print(f'Final file size: {os.path.getsize(csv_filename)} bytes')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(csv_filename):\n",
    "        print(f\"CSV file exists. Size: {os.path.getsize(csv_filename)} bytes\")\n",
    "    else:\n",
    "        print(\"CSV file does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
